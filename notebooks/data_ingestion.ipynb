{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35c716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RetailDemandIngestion\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c86aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pdf = pd.read_excel(\"../data/raw/Online_Retail.xlsx\")\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9597b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"retail_sales\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    Country,\n",
    "    COUNT(DISTINCT InvoiceNo) AS total_orders,\n",
    "    SUM(Quantity) AS total_quantity,\n",
    "    ROUND(SUM(Quantity * UnitPrice), 2) AS revenue\n",
    "FROM retail_sales\n",
    "GROUP BY Country\n",
    "ORDER BY revenue DESC\n",
    "\"\"\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cbc1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    InvoiceNo,\n",
    "    COUNT(*) AS items_in_invoice\n",
    "FROM retail_sales\n",
    "GROUP BY InvoiceNo\n",
    "ORDER BY items_in_invoice DESC\n",
    "\"\"\").show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_sales_df = spark.sql(\"\"\"\n",
    "SELECT Country, SUM(Quantity * UnitPrice) AS revenue\n",
    "FROM retail_sales\n",
    "GROUP BY Country\n",
    "\"\"\")\n",
    "\n",
    "country_sales_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e69505",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    Country,\n",
    "    COUNT(*) AS rows_count\n",
    "FROM retail_sales\n",
    "GROUP BY Country\n",
    "ORDER BY rows_count DESC\n",
    "\"\"\").show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cf7170",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"table_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0109d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM retail_sales\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc19a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"retail_sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7823434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbaf4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_clean = df.select([\n",
    "    col(c).alias(c.lower().replace(\" \", \"_\")) for c in df.columns\n",
    "])\n",
    "\n",
    "df_clean.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df990328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, when\n",
    "\n",
    "df_clean.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) for c in df_clean.columns\n",
    "]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf68d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df_clean.filter(\n",
    "    (col(\"quantity\") > 0) &\n",
    "    (col(\"unitprice\") > 0) &\n",
    "    col(\"invoiceno\").isNotNull() &\n",
    "    col(\"invoicedate\").isNotNull()\n",
    ")\n",
    "\n",
    "df_valid.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8280530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_valid = df_valid.withColumn(\n",
    "    \"customerid\",\n",
    "    when(col(\"customerid\").isNull(), -1).otherwise(col(\"customerid\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.createOrReplaceTempView(\"retail_sales_clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT country, COUNT(*) AS rows_count\n",
    "FROM retail_sales_clean\n",
    "GROUP BY country\n",
    "ORDER BY rows_count DESC\n",
    "\"\"\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba44949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "df_enriched = df_valid.withColumn(\n",
    "    \"revenue\",\n",
    "    round(col(\"quantity\") * col(\"unitprice\"), 2)\n",
    ")\n",
    "\n",
    "df_enriched.select(\"quantity\", \"unitprice\", \"revenue\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"../data/processed/retail_sales_clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"../data/processed/retail_sales_clean\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2369ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched.createOrReplaceTempView(\"retail_sales_analytics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ab7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9cac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_sales = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    country,\n",
    "    COUNT(DISTINCT invoiceno) AS total_orders,\n",
    "    SUM(quantity) AS total_quantity,\n",
    "    ROUND(SUM(revenue), 2) AS total_revenue\n",
    "FROM retail_sales_analytics\n",
    "GROUP BY country\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "country_sales.show(10, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9182ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_demand = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    DATE(invoicedate) AS sales_date,\n",
    "    SUM(quantity) AS daily_quantity,\n",
    "    ROUND(SUM(revenue), 2) AS daily_revenue\n",
    "FROM retail_sales_analytics\n",
    "GROUP BY DATE(invoicedate)\n",
    "ORDER BY sales_date\n",
    "\"\"\")\n",
    "\n",
    "daily_demand.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7016313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_demand = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    stockcode,\n",
    "    description,\n",
    "    SUM(quantity) AS total_quantity_sold,\n",
    "    ROUND(SUM(revenue), 2) AS total_revenue\n",
    "FROM retail_sales_analytics\n",
    "GROUP BY stockcode, description\n",
    "ORDER BY total_quantity_sold DESC\n",
    "\"\"\")\n",
    "\n",
    "product_demand.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_sales.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"../data/processed/country_sales\")\n",
    "\n",
    "daily_demand.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"../data/processed/daily_demand\")\n",
    "\n",
    "product_demand.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"../data/processed/product_demand\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a4ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"../data/processed/daily_demand\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc963b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek, weekofyear, month, year\n",
    "\n",
    "daily_features = daily_demand \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(\"sales_date\")) \\\n",
    "    .withColumn(\"week_of_year\", weekofyear(\"sales_date\")) \\\n",
    "    .withColumn(\"month\", month(\"sales_date\")) \\\n",
    "    .withColumn(\"year\", year(\"sales_date\"))\n",
    "\n",
    "daily_features.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "window_spec = Window.orderBy(\"sales_date\")\n",
    "\n",
    "daily_features = daily_features \\\n",
    "    .withColumn(\"lag_1_day\", lag(\"daily_quantity\", 1).over(window_spec)) \\\n",
    "    .withColumn(\"lag_7_day\", lag(\"daily_quantity\", 7).over(window_spec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cb0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_features.select(\n",
    "    \"sales_date\", \"daily_quantity\", \"lag_1_day\", \"lag_7_day\"\n",
    ").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc19a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "rolling_window = window_spec.rowsBetween(-7, -1)\n",
    "\n",
    "daily_features = daily_features.withColumn(\n",
    "    \"rolling_7_day_avg\",\n",
    "    avg(\"daily_quantity\").over(rolling_window)\n",
    ")\n",
    "\n",
    "daily_features.select(\n",
    "    \"sales_date\", \"daily_quantity\", \"rolling_7_day_avg\"\n",
    ").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b141e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_features_clean = daily_features.dropna()\n",
    "daily_features_clean.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a16880",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_features_clean.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"../data/processed/daily_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd67fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"../data/processed/daily_features\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ba7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = spark.read.parquet(\"../data/processed/daily_features\")\n",
    "features_df.printSchema()\n",
    "features_df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb14fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = features_df.orderBy(\"sales_date\").toPandas()\n",
    "pdf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8524ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"lag_1_day\",\n",
    "    \"lag_7_day\",\n",
    "    \"rolling_7_day_avg\",\n",
    "    \"day_of_week\",\n",
    "    \"week_of_year\",\n",
    "    \"month\"\n",
    "]\n",
    "\n",
    "X = pdf[feature_cols]\n",
    "y = pdf[\"daily_quantity\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea8262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(pdf) * 0.8)\n",
    "\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cdfaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce38446",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "results = pdf.iloc[split_index:][[\"sales_date\"]].copy()\n",
    "results[\"actual_demand\"] = y_test.values\n",
    "results[\"predicted_demand\"] = y_pred\n",
    "\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076aeb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "mae, rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd6cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"../data/processed/demand_forecast_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013f315",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e3e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514326fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c631de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_tensor[:split_index]\n",
    "X_test  = X_tensor[split_index:]\n",
    "y_train = y_tensor[:split_index]\n",
    "y_test  = y_tensor[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DemandForecastNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "model = DemandForecastNet(X_train.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41449aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2fdb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_torch = model(X_test)\n",
    "\n",
    "y_pred_torch = y_pred_torch.numpy().flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41de459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "mae_torch = mean_absolute_error(y_test.numpy(), y_pred_torch)\n",
    "rmse_torch = np.sqrt(mean_squared_error(y_test.numpy(), y_pred_torch))\n",
    "\n",
    "mae_torch, rmse_torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4d445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Model\": [\"Linear Regression\", \"PyTorch Neural Net\"],\n",
    "    \"MAE\": [mae, mae_torch],\n",
    "    \"RMSE\": [rmse, rmse_torch]\n",
    "})\n",
    "\n",
    "comparison\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
