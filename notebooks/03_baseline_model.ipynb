{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da4f52d3",
   "metadata": {},
   "source": [
    "\n",
    "## Baseline Modeling\n",
    "\n",
    "This notebook trains a baseline Linear Regression model using Spark MLlib.\n",
    "To validate results, a Pandas + scikit-learn implementation is also used\n",
    "to compare performance on an in-memory dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8f35f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/16 17:47:13 WARN Utils: Your hostname, MacBook-Air-3.local resolves to a loopback address: 127.0.0.1; using 10.0.0.22 instead (on interface en0)\n",
      "26/01/16 17:47:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/16 17:47:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/01/16 17:47:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "26/01/16 17:47:13 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RetailDemandBaselineModel\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ae4c096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sales_date: date (nullable = true)\n",
      " |-- daily_quantity: long (nullable = true)\n",
      " |-- daily_revenue: double (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- lag_1_day: long (nullable = true)\n",
      " |-- lag_7_day: long (nullable = true)\n",
      " |-- rolling_7_day_avg: double (nullable = true)\n",
      "\n",
      "+----------+--------------+------------------+-----------+------------+-----+----+---------+---------+------------------+\n",
      "|sales_date|daily_quantity|     daily_revenue|day_of_week|week_of_year|month|year|lag_1_day|lag_7_day| rolling_7_day_avg|\n",
      "+----------+--------------+------------------+-----------+------------+-----+----+---------+---------+------------------+\n",
      "|2010-12-09|         19930| 53586.18000000004|          5|          49|   12|2010|    23117|    26919| 23004.14285714286|\n",
      "|2010-12-10|         21097| 59182.92000000025|          6|          49|   12|2010|    19930|    31329|22005.714285714286|\n",
      "|2010-12-12|         10603|17329.070000000047|          1|          49|   12|2010|    21097|    16199|           20544.0|\n",
      "|2010-12-13|         17727|  38006.7100000001|          2|          50|   12|2010|    10603|    16450|19744.571428571428|\n",
      "|2010-12-14|         20284|45254.730000000185|          3|          50|   12|2010|    17727|    21795|           19927.0|\n",
      "+----------+--------------+------------------+-----------+------------+-----+----+---------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"../data/processed/daily_features\")\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f8dec51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "298"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model = df.dropna()\n",
    "df_model.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6d9a764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n",
      "+--------------------+-------+\n",
      "|            features|  label|\n",
      "+--------------------+-------+\n",
      "|[23117.0,26919.0,...|19930.0|\n",
      "|[19930.0,31329.0,...|21097.0|\n",
      "|[21097.0,16199.0,...|10603.0|\n",
      "|[10603.0,16450.0,...|17727.0|\n",
      "|[17727.0,21795.0,...|20284.0|\n",
      "+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"lag_1_day\",\n",
    "        \"lag_7_day\",\n",
    "        \"rolling_7_day_avg\",\n",
    "        \"day_of_week\",\n",
    "        \"week_of_year\",\n",
    "        \"month\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_final = assembler.transform(df_model) \\\n",
    "    .select(\n",
    "        \"features\",\n",
    "        col(\"daily_quantity\").cast(\"double\").alias(\"label\")\n",
    "    ) \\\n",
    "    .dropna()\n",
    "\n",
    "df_final.printSchema()\n",
    "df_final.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f400498f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------------+-----------+------------+-----+\n",
      "|lag_1_day|lag_7_day| rolling_7_day_avg|day_of_week|week_of_year|month|\n",
      "+---------+---------+------------------+-----------+------------+-----+\n",
      "|    23117|    26919| 23004.14285714286|          5|          49|   12|\n",
      "|    19930|    31329|22005.714285714286|          6|          49|   12|\n",
      "|    21097|    16199|           20544.0|          1|          49|   12|\n",
      "|    10603|    16450|19744.571428571428|          2|          50|   12|\n",
      "|    17727|    21795|           19927.0|          3|          50|   12|\n",
      "+---------+---------+------------------+-----------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model.select(\n",
    "    \"lag_1_day\",\n",
    "    \"lag_7_day\",\n",
    "    \"rolling_7_day_avg\",\n",
    "    \"day_of_week\",\n",
    "    \"week_of_year\",\n",
    "    \"month\"\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70da7d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            features|  label|\n",
      "+--------------------+-------+\n",
      "|[23117.0,26919.0,...|19930.0|\n",
      "|[19930.0,31329.0,...|21097.0|\n",
      "|[21097.0,16199.0,...|10603.0|\n",
      "|[10603.0,16450.0,...|17727.0|\n",
      "|[17727.0,21795.0,...|20284.0|\n",
      "+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"lag_1_day\",\n",
    "        \"lag_7_day\",\n",
    "        \"rolling_7_day_avg\",\n",
    "        \"day_of_week\",\n",
    "        \"week_of_year\",\n",
    "        \"month\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_final = assembler.transform(df_model) \\\n",
    "    .select(\n",
    "        \"features\",\n",
    "        col(\"daily_quantity\").cast(\"double\").alias(\"label\")\n",
    "    ) \\\n",
    "    .dropna()\n",
    "\n",
    "df_final.show(5)\n",
    "df_final.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "828b0dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 46)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train_df.count(), test_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b46ef632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()\n",
    "test_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "619371da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c48c61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/16 17:47:16 WARN Instrumentation: [08f39427] regParam is zero, which might cause numerical instability and overfitting.\n",
      "26/01/16 17:47:16 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "26/01/16 17:47:16 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "26/01/16 17:47:16 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\"\n",
    ")\n",
    "\n",
    "lr_model = lr.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30d24ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|  label|        prediction|\n",
      "+-------+------------------+\n",
      "|13595.0| 7498.243742078078|\n",
      "|13415.0| 7706.829347421407|\n",
      "|14940.0|19145.421937825115|\n",
      "|12263.0|  9322.12482314435|\n",
      "|21589.0|11194.177199416943|\n",
      "|10244.0|17908.696812911323|\n",
      "|13501.0| 7107.303057185462|\n",
      "|19771.0| 8602.501111363217|\n",
      "| 3431.0| 3978.763829643108|\n",
      "|12145.0| 9648.245210404344|\n",
      "+-------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lr_model.transform(test_df)\n",
    "predictions.select(\"label\", \"prediction\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aed4fb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13222.980446660433, 7630.990098285122)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "rmse_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"mae\"\n",
    ")\n",
    "\n",
    "rmse_spark = rmse_evaluator.evaluate(predictions)\n",
    "mae_spark = mae_evaluator.evaluate(predictions)\n",
    "\n",
    "rmse_spark, mae_spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e04bdf",
   "metadata": {},
   "source": [
    "## scikit-learn Baseline (Validation)\n",
    "\n",
    "To validate Spark MLlib results, the same feature set is used to train a\n",
    "Linear Regression model using Pandas and scikit-learn on an in-memory dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffc9b999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales_date</th>\n",
       "      <th>daily_quantity</th>\n",
       "      <th>daily_revenue</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>lag_1_day</th>\n",
       "      <th>lag_7_day</th>\n",
       "      <th>rolling_7_day_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-12-09</td>\n",
       "      <td>19930</td>\n",
       "      <td>53586.18</td>\n",
       "      <td>5</td>\n",
       "      <td>49</td>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "      <td>23117</td>\n",
       "      <td>26919</td>\n",
       "      <td>23004.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-12-10</td>\n",
       "      <td>21097</td>\n",
       "      <td>59182.92</td>\n",
       "      <td>6</td>\n",
       "      <td>49</td>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "      <td>19930</td>\n",
       "      <td>31329</td>\n",
       "      <td>22005.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-12-12</td>\n",
       "      <td>10603</td>\n",
       "      <td>17329.07</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "      <td>21097</td>\n",
       "      <td>16199</td>\n",
       "      <td>20544.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-12-13</td>\n",
       "      <td>17727</td>\n",
       "      <td>38006.71</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "      <td>10603</td>\n",
       "      <td>16450</td>\n",
       "      <td>19744.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-12-14</td>\n",
       "      <td>20284</td>\n",
       "      <td>45254.73</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "      <td>17727</td>\n",
       "      <td>21795</td>\n",
       "      <td>19927.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sales_date  daily_quantity  daily_revenue  day_of_week  week_of_year  \\\n",
       "0  2010-12-09           19930       53586.18            5            49   \n",
       "1  2010-12-10           21097       59182.92            6            49   \n",
       "2  2010-12-12           10603       17329.07            1            49   \n",
       "3  2010-12-13           17727       38006.71            2            50   \n",
       "4  2010-12-14           20284       45254.73            3            50   \n",
       "\n",
       "   month  year  lag_1_day  lag_7_day  rolling_7_day_avg  \n",
       "0     12  2010      23117      26919       23004.142857  \n",
       "1     12  2010      19930      31329       22005.714286  \n",
       "2     12  2010      21097      16199       20544.000000  \n",
       "3     12  2010      10603      16450       19744.571429  \n",
       "4     12  2010      17727      21795       19927.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = df_model.orderBy(\"sales_date\").toPandas()\n",
    "pdf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b6a99e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"lag_1_day\",\n",
    "    \"lag_7_day\",\n",
    "    \"rolling_7_day_avg\",\n",
    "    \"day_of_week\",\n",
    "    \"week_of_year\",\n",
    "    \"month\"\n",
    "]\n",
    "\n",
    "X = pdf[feature_cols]\n",
    "y = pdf[\"daily_quantity\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b74a07f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(pdf) * 0.8)\n",
    "\n",
    "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3c302ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "sk_model = LinearRegression()\n",
    "sk_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_sk = sk_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "888015fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(14363.866894630732), 10220.03884824338)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "mae_sk = mean_absolute_error(y_test, y_pred_sk)\n",
    "rmse_sk = np.sqrt(mean_squared_error(y_test, y_pred_sk))\n",
    "\n",
    "rmse_sk, mae_sk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cfd173a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spark Linear Regression</td>\n",
       "      <td>13222.980447</td>\n",
       "      <td>7630.990098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scikit-learn Linear Regression</td>\n",
       "      <td>14363.866895</td>\n",
       "      <td>10220.038848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model          RMSE           MAE\n",
       "0         Spark Linear Regression  13222.980447   7630.990098\n",
       "1  scikit-learn Linear Regression  14363.866895  10220.038848"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Model\": [\"Spark Linear Regression\", \"scikit-learn Linear Regression\"],\n",
    "    \"RMSE\": [rmse_spark, rmse_sk],\n",
    "    \"MAE\": [mae_spark, mae_sk]\n",
    "})\n",
    "\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abfb3a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238, 60)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2399553b",
   "metadata": {},
   "source": [
    "### Comparison Summary\n",
    "\n",
    "Both Spark MLlib and scikit-learn Linear Regression models show comparable\n",
    "performance. Minor differences are expected due to distributed execution\n",
    "(Spark) versus in-memory computation (scikit-learn).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba9e31f",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Spark Linear Regression outperformed the scikit-learn implementation.\n",
    "This difference is expected due to variations in numerical optimization,\n",
    "feature vectorization, and execution environment (distributed Spark vs\n",
    "in-memory scikit-learn).\n",
    "\n",
    "The scikit-learn model is included as a validation baseline to confirm\n",
    "trend consistency rather than exact metric parity.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
