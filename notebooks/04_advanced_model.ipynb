{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04486eef",
   "metadata": {},
   "source": [
    "# Step 4: Advanced Modeling â€“ RandomForest Regressor\n",
    "\n",
    "This notebook trains a RandomForest regression model using Spark MLlib\n",
    "to capture non-linear demand patterns and compares its performance\n",
    "against the baseline Linear Regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12047a5d",
   "metadata": {},
   "source": [
    "Start Spark & Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4bb05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/19 10:31:00 WARN Utils: Your hostname, MacBook-Air-3.local resolves to a loopback address: 127.0.0.1; using 10.0.0.22 instead (on interface en0)\n",
      "26/01/19 10:31:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/19 10:31:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/01/19 10:31:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "26/01/19 10:31:01 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "26/01/19 10:31:01 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-----------+------------+-----+----+-----+-----+------------------+------------------+\n",
      "|      date|daily_quantity|day_of_week|week_of_year|month|year|lag_1|lag_7|         rolling_7|        rolling_14|\n",
      "+----------+--------------+-----------+------------+-----+----+-----+-----+------------------+------------------+\n",
      "|2010-12-09|         19930|          5|          49|   12|2010|23117|26919| 23004.14285714286| 23004.14285714286|\n",
      "|2010-12-10|         21097|          6|          49|   12|2010|19930|31329|22005.714285714286|         22619.875|\n",
      "|2010-12-12|         10603|          1|          49|   12|2010|21097|16199|           20544.0|22450.666666666668|\n",
      "|2010-12-13|         17727|          2|          50|   12|2010|10603|16450|19744.571428571428|           21265.9|\n",
      "|2010-12-14|         20284|          3|          50|   12|2010|17727|21795|           19927.0| 20944.18181818182|\n",
      "+----------+--------------+-----------+------------+-----+----+-----+-----+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RetailDemandAdvancedModel\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_model = spark.read.parquet(\"../data/processed/daily_features\")\n",
    "df_model.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4335d07",
   "metadata": {},
   "source": [
    "Verify Columns (Sanity Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48abe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- daily_quantity: long (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- lag_1: long (nullable = true)\n",
      " |-- lag_7: long (nullable = true)\n",
      " |-- rolling_7: double (nullable = true)\n",
      " |-- rolling_14: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754e3b06",
   "metadata": {},
   "source": [
    "Assemble Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b106dc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            features|  label|\n",
      "+--------------------+-------+\n",
      "|[23117.0,26919.0,...|19930.0|\n",
      "|[19930.0,31329.0,...|21097.0|\n",
      "|[21097.0,16199.0,...|10603.0|\n",
      "|[10603.0,16450.0,...|17727.0|\n",
      "|[17727.0,21795.0,...|20284.0|\n",
      "+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"lag_1\",\n",
    "        \"lag_7\",\n",
    "        \"rolling_7\",\n",
    "        \"rolling_14\",\n",
    "        \"day_of_week\",\n",
    "        \"week_of_year\",\n",
    "        \"month\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_final = assembler.transform(df_model) \\\n",
    "    .select(\n",
    "        \"features\",\n",
    "        col(\"daily_quantity\").cast(\"double\").alias(\"label\")\n",
    "    ) \\\n",
    "    .dropna()\n",
    "\n",
    "df_final.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6c5742",
   "metadata": {},
   "source": [
    "Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39d0ea63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 252\n",
      "Test rows: 46\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(\"Train rows:\", train_df.count())\n",
    "print(\"Test rows:\", test_df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de46239",
   "metadata": {},
   "source": [
    "Train RandomForest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63838241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=50,\n",
    "    maxDepth=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rf_model = rf.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a264af",
   "metadata": {},
   "source": [
    "Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de9af1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|  label|        prediction|\n",
      "+-------+------------------+\n",
      "|13595.0|13517.511094932686|\n",
      "|13415.0|13850.896159152366|\n",
      "|14940.0|16557.181667683344|\n",
      "|12263.0| 16219.34044287519|\n",
      "|21589.0|13373.532894876222|\n",
      "+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predictions = rf_model.transform(test_df)\n",
    "rf_predictions.select(\"label\", \"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94857f95",
   "metadata": {},
   "source": [
    "Evaluate RandomForest (RMSE & MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b72c6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12348.630176079307, 6313.760869699056)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "rmse_eval = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "mae_eval = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"mae\"\n",
    ")\n",
    "\n",
    "rmse_rf = rmse_eval.evaluate(rf_predictions)\n",
    "mae_rf = mae_eval.evaluate(rf_predictions)\n",
    "\n",
    "rmse_rf, mae_rf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c2311",
   "metadata": {},
   "source": [
    "Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "756cc36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions \\\n",
    "    .select(\"label\", \"prediction\") \\\n",
    "    .toPandas() \\\n",
    "    .to_csv(\"../data/processed/rf_predictions.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
